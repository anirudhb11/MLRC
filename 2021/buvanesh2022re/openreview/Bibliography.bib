@article{liu_variance_2020,
	title = {On the {Variance} of the {Adaptive} {Learning} {Rate} and {Beyond}},
	url = {http://arxiv.org/abs/1908.03265},
	abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.},
	journal = {arXiv:1908.03265 [cs, stat]},
	author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
	month = apr,
	year = {2020},
	note = {arXiv: 1908.03265},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2020. Fix several typos in the previous version},
	file = {arXiv Fulltext PDF:/home/madhur/Zotero/storage/UNWVRJBD/Liu et al. - 2020 - On the Variance of the Adaptive Learning Rate and .pdf:application/pdf;arXiv.org Snapshot:/home/madhur/Zotero/storage/YLITA6J2/1908.html:text/html},
}

@article{chen_closing_2020,
	title = {Closing the {Generalization} {Gap} of {Adaptive} {Gradient} {Methods} in {Training} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1806.06763},
	abstract = {Adaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, despite the nice property of fast convergence, have been observed to generalize worse than stochastic gradient descent (SGD) with momentum in training deep neural networks. This leaves how to close the generalization gap of adaptive gradient methods an open problem. In this work, we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes "over adapted". We design a new algorithm, called Partially adaptive momentum estimation method, which unifies the Adam/Amsgrad with SGD by introducing a partial adaptive parameter \$p\$, to achieve the best from both worlds. We also prove the convergence rate of our proposed algorithm to a stationary point in the stochastic nonconvex optimization setting. Experiments on standard benchmarks show that our proposed algorithm can maintain a fast convergence rate as Adam/Amsgrad while generalizing as well as SGD in training deep neural networks. These results would suggest practitioners pick up adaptive gradient methods once again for faster training of deep neural networks.},
	journal = {arXiv:1806.06763 [cs, stat]},
	author = {Chen, Jinghui and Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
	month = jun,
	year = {2020},
	note = {arXiv: 1806.06763},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 4 figures, 4 tables. In IJCAI 2020},
	file = {arXiv Fulltext PDF:/home/madhur/Zotero/storage/677RT8JG/Chen et al. - 2020 - Closing the Generalization Gap of Adaptive Gradien.pdf:application/pdf;arXiv.org Snapshot:/home/madhur/Zotero/storage/IHMGPNI9/1806.html:text/html},
}

@article{Povey:192584,
      title = {The Kaldi Speech Recognition Toolkit},
      author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and  Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and  Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and  Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and  Vesely, Karel},
      publisher = {IEEE Signal Processing Society},
      year = {2011},
      note = {IEEE Catalog No.: CFP11SRW-USB},
      abstract = {We describe the design of Kaldi, a free, open-source  toolkit for speech recognition research. Kaldi provides a  speech recognition system based on finite-state transducers  (using the freely available OpenFst), together with  detailed documentation and scripts for building complete  recognition systems. Kaldi is written is C++, and the core  library supports modeling of arbitrary phonetic-context  sizes, acoustic modeling with subspace Gaussian mixture  models (SGMM) as well as standard Gaussian mixture models,  together with all commonly used linear and affine  transforms. Kaldi is released under the Apache License  v2.0, which is highly nonrestrictive, making it suitable  for a wide community of users.},
      url = {http://infoscience.epfl.ch/record/192584},
}

@inproceedings{lin_lstm_2019,
	title = {{LSTM} {Based} {Similarity} {Measurement} with {Spectral} {Clustering} for {Speaker} {Diarization}},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/1388.html},
	doi = {10.21437/Interspeech.2019-1388},
	abstract = {More and more neural network approaches have achieved considerable improvement upon submodules of speaker diarization system, including speaker change detection and segment-wise speaker embedding extraction. Still, in the clustering stage, traditional algorithms like probabilistic linear discriminant analysis (PLDA) are widely used for scoring the similarity between two speech segments. In this paper, we propose a supervised method to measure the similarity matrix between all segments of an audio recording with sequential bidirectional long shortterm memory networks (Bi-LSTM). Spectral clustering is applied on top of the similarity matrix to further improve the performance. Experimental results show that our system signiﬁcantly outperforms the state-of-the-art methods and achieves a diarization error rate of 6.63\% on the NIST SRE 2000 CALLHOME database.},
	language = {en},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Lin, Qingjian and Yin, Ruiqing and Li, Ming and Bredin, Hervé and Barras, Claude},
	month = sep,
	year = {2019},
	pages = {366--370},
	file = {Lin et al. - 2019 - LSTM Based Similarity Measurement with Spectral Cl.pdf:/home/madhur/Zotero/storage/2Y6XPW3U/Lin et al. - 2019 - LSTM Based Similarity Measurement with Spectral Cl.pdf:application/pdf},
}

@article{CIFAR,
    title = {Geoffrey Hinton Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report},
    year = {2009},
    url ={https://www.cs.toronto.edu/~kriz/cifar.html}
}

@inproceedings{Imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{PTB,
author = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
title = {Building a Large Annotated Corpus of English: The Penn Treebank},
year = {1993},
issue_date = {June 1993},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {19},
number = {2},
issn = {0891-2017},
journal = {Comput. Linguist.},
month = jun,
pages = {313–330},
numpages = {18}
}

@article{WikiText_2,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  journal   = {CoRR},
  volume    = {abs/1609.07843},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07843},
  archivePrefix = {arXiv},
  eprint    = {1609.07843},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MerityXBS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{VGG,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@INPROCEEDINGS{Resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}
  
@INPROCEEDINGS{Densenet,
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Densely Connected Convolutional Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2261-2269},
  doi={10.1109/CVPR.2017.243}
}
 
@article{xiaolei_LSTM,
title = {Long short-term memory neural network for traffic speed prediction using remote microwave sensor data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {54},
pages = {187-197},
year = {2015},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15000935},
author = {Xiaolei Ma and Zhimin Tao and Yinhai Wang and Haiyang Yu and Yunpeng Wang},
keywords = {Neural networks, Long short-term neural network, Traffic speed prediction, Remote microwave detector data},
abstract = {Neural networks have been extensively applied to short-term traffic prediction in the past years. This study proposes a novel architecture of neural networks, Long Short-Term Neural Network (LSTM NN), to capture nonlinear traffic dynamic in an effective manner. The LSTM NN can overcome the issue of back-propagated error decay through memory blocks, and thus exhibits the superior capability for time series prediction with long temporal dependency. In addition, the LSTM NN can automatically determine the optimal time lags. To validate the effectiveness of LSTM NN, travel speed data from traffic microwave detectors in Beijing are used for model training and testing. A comparison with different topologies of dynamic neural networks as well as other prevailing parametric and nonparametric algorithms suggests that LSTM NN can achieve the best prediction performance in terms of both accuracy and stability.}
}

@misc{GAN,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{WGAN,
      title={Wasserstein GAN}, 
      author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
      year={2017},
      eprint={1701.07875},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{WGAN-GP,
      title={Improved Training of Wasserstein GANs}, 
      author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
      year={2017},
      eprint={1704.00028},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{SN-GAN,
  author    = {Takeru Miyato and
               Toshiki Kataoka and
               Masanori Koyama and
               Yuichi Yoshida},
  title     = {Spectral Normalization for Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1802.05957},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05957},
  archivePrefix = {arXiv},
  eprint    = {1802.05957},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05957.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhuang_adabelief_2020,
	title = {{AdaBelief} {Optimizer}: {Adapting} {Stepsizes} by the {Belief} in {Observed} {Gradients}},
	volume = {33},
	shorttitle = {{AdaBelief} {Optimizer}},
	url = {https://papers.nips.cc/paper/2020/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract.html},
	language = {en},
	urldate = {2021-06-09},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C. and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
	year = {2020},
	pages = {18795--18806},
	file = {Snapshot:/home/madhur/Zotero/storage/3LRDXQVB/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract.html:text/html;Full Text PDF:/home/madhur/Zotero/storage/J54V68YS/Zhuang et al. - 2020 - AdaBelief Optimizer Adapting Stepsizes by the Bel.pdf:application/pdf;AdaBelief_NIPS_supplementary.pdf:/home/madhur/Zotero/storage/6WGC55H2/AdaBelief_NIPS_supplementary.pdf:application/pdf},
}

@misc{Weight_decay_Reg,
      title={Three Mechanisms of Weight Decay Regularization}, 
      author={Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},
      year={2018},
      eprint={1810.12281},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{SGD,
  added-at = {2008-10-07T16:03:39.000+0200},
  author = {Robbins, H. and Monro, S.},
  biburl = {https://www.bibsonomy.org/bibtex/2cc1b9aa8927ac4952e93f34094a3eaaf/brefeld},
  interhash = {93d54534a08c30eda9e34d1def03ffa3},
  intrahash = {cc1b9aa8927ac4952e93f34094a3eaaf},
  journal = {Annals of Mathematical Statistics},
  keywords = {imported},
  pages = {400-407},
  timestamp = {2008-10-07T16:03:40.000+0200},
  title = {A stochastic approximation method},
  volume = 22,
  year = 1951
}

@misc{RMSProp,
      title={Generating Sequences With Recurrent Neural Networks}, 
      author={Alex Graves},
      year={2014},
      eprint={1308.0850},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{Adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{AdamW,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Yogi,
author = {Zaheer, Manzil and Reddi, Sashank J. and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
title = {Adaptive Methods for Nonconvex Optimization},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Adaptive gradient methods that rely on scaling gradients down by the square root of
exponential moving averages of past squared gradients, such RMSPROP, ADAM, ADADELTA
have found wide application in optimizing the nonconvex problems that arise in deep
learning. However, it has been recently demonstrated that such methods can fail to
converge even in simple convex optimization settings. In this work, we provide a new
analysis of such methods applied to nonconvex stochastic optimization problems, characterizing
the effect of increasing minibatch size. Our analysis shows that under this scenario
such methods do converge to stationarity up to the statistical limit of variance in
the stochastic gradients (scaled by a constant factor). In particular, our result
implies that increasing minibatch sizes enables convergence, thus providing a way
to circumvent the nonconvergence issues. Furthermore, we provide a new adaptive optimization
algorithm, YOGI, which controls the increase in effective learning rate, leading to
even better performance with similar theoretical guarantees on convergence. Extensive
experiments show that YOGI with very little hyperparameter tuning outperforms methods
such as ADAM in several challenging machine learning tasks.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {9815–9825},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@misc{MSVAG,
      title={Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients}, 
      author={Lukas Balles and Philipp Hennig},
      year={2020},
      eprint={1705.07774},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Fromage, 
  title={On the distance between two neural networks and the stability of learning},
  author={Jeremy Bernstein and Arash Vahdat and Yisong Yue and Ming-Yu Liu},
  booktitle = {Neural Information Processing Systems},
  year={2020}
}

@misc{AdaBound,
      title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate}, 
      author={Liangchen Luo and Yuanhao Xiong and Yan Liu and Xu Sun},
      year={2019},
      eprint={1902.09843},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Apollo,
      title={Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization}, 
      author={Xuezhe Ma},
      year={2021},
      eprint={2009.13586},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{NAG,
author="NESTEROV, Y. E.",
title="A method for solving the convex programming problem with convergence rate $O(1/k^2)$",
journal="Dokl. Akad. Nauk SSSR",
ISSN="",
publisher="",
year="1983",
month="",
volume="269",
number="",
pages="543-547",
URL="https://ci.nii.ac.jp/naid/10029946121/en/",
DOI="",
}

@InProceedings{Momentum, title = {On the importance of initialization and momentum in deep learning}, author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton}, booktitle = {Proceedings of the 30th International Conference on Machine Learning}, pages = {1139--1147}, year = {2013}, editor = {Sanjoy Dasgupta and David McAllester}, volume = {28}, number = {3}, series = {Proceedings of Machine Learning Research}, address = {Atlanta, Georgia, USA}, month = {17--19 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v28/sutskever13.pdf}, url = {http://proceedings.mlr.press/v28/sutskever13.html}, abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods. } }

@misc{AdaDelta,
      title={ADADELTA: An Adaptive Learning Rate Method}, 
      author={Matthew D. Zeiler},
      year={2012},
      eprint={1212.5701},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{DQN_RL,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{padam,
      title={ICLR Reproducibility Challenge Report (Padam : Closing The Generalization Gap Of Adaptive Gradient Methods in Training Deep Neural Networks)}, 
      author={Harshal Mittal and Kartikey Pandey and Yash Kant},
      year={2019},
      eprint={1901.09517},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{LSTM,
  title={Long Short-Term Memory},
  author={S. Hochreiter and J. Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@article{ng_deep_nodate,
	title = {Diagnosing Bias vs. Variance},
	shorttitle = {{AdaBelief} {Optimizer}},
	url = {https://www.coursera.org/learn/machine-learning/lecture/yCAup/diagnosing-bias-vs-variance},
	language = {en},
	journal = {Coursera},
	author = {Ng, Andrew},
	year = {2017},
}