% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/anirudhb11/Adabelief-Optimizer-RC}
\def \codeDOI{}
\def \codeSWH{swh:1:dir:53eeebe14e9d02d912fc3c58c375b5095e8db941}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha}
\def \editorORCID{}
\def \reviewerINAME{Anonymous Reviewers}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{}
\def \dateACCEPTED{11 April 2022}
\def \datePUBLISHED{}
\def \articleTITLE{[Re] AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2021}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2022}
\def \reviewURL{https://openreview.net/forum?id=B9gDnMmn0t}
\def \articleABSTRACT{Optimization is of prime importance to machine learning and is an active area of research. A better optimization algorithm helps in achieving better optima faster. AdaBelief is an optimizer that claims to have 1) fast convergence as in adaptive methods, 2) good generalization as in SGD, and 3) training stability. This report contains the experiments to validate these claims and test the effectiveness of AdaBelief. We first perform the experiments from AdaBelief's paper which cover a variety of datasets spanning multiple domains including Image Classification, Language Modeling, Generative Modeling, and Reinforcement Learning. We perform several analyses targeted toward AdaBelief's claims and find that the convergence speed and training stability of AdaBelief is comparable to that of adaptive gradient optimizers. However, AdaBelief does not generalize as well as SGD. Nevertheless, it is a promising optimizer combining the best of both worlds ‚Äê accelerated and adaptive gradient methods.}
\def \replicationCITE{Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, James S. Duncan. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients (NeurIPS 2020).}
\def \replicationBIB{zhuang_adabelief_2020}
\def \replicationURL{https://arxiv.org/abs/2010.07468}
\def \replicationDOI{https://doi.org/10.48550/arXiv.2010.07468}
\def \contactNAME{Anirudh Buvanesh}
\def \contactEMAIL{anirudhb1102@gmail.com}
\def \articleKEYWORDS{rescience c, machine learning, optimizers, image classification, language modelling, generative adversarial networks}
\def \journalNAME{ReScience C}
\def \journalVOLUME{9}
\def \journalISSUE{1}
\def \articleNUMBER{}
\def \articleDOI{10.0000/zenodo.0000000}
\def \authorsFULL{Anirudh Buvanesh and Madhur Panwar}
\def \authorsABBRV{A. Buvanesh and M. Panwar}
\def \authorsSHORT{Buvanesh and Panwar}
\title{\articleTITLE}
\date{}
\author[1,2,\orcid{0000-0003-1910-2253}]{Anirudh Buvanesh}
\author[1,2,\orcid{0000-0002-0053-733X}]{Madhur Panwar}
\affil[1]{Birla Institute of Technology and Science, Pilani (BITS Pilani), Pilani, India}
\affil[2]{Equal contribution}
